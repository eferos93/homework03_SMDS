---
title: "Homework 3"
author: "Group H - A. Spagnolo, A. Vegliach, V. Plesco, E. Fabrici"
date: "13/05/2020"
output:
  html_document:
    toc: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
subtitle: 
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
library(MASS)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

## ***LEC*** Exercises

**Exercise 1**

Compute the bootstrap-based confidence interval for the `score` dataset using the studentized method.

**Exercise 2**

Compute bootstrap-based confidence intervals for the `score` dataset using the `boot` package.
 

## ***LAB*** Exercises

**Exercise 1**

Use `nlm` to compute the variance for the estimator $\hat{\omega} = (\log( \hat{\gamma} ), \log( \hat{\beta} )$  and `optimHess` for the variance of $\hat{\theta} = (\hat{\gamma}, \hat{\beta})$


**Exercise 2**

The Wald confidence interval with level $1-\alpha$ is defined as:
$$\hat{\gamma} \pm z_{1-\frac{\alpha}{2}} j_P(\hat{\gamma})^{-\frac{1}{2}}$$
Compute the Wald confidence interval of level 0.95 and plot the results.

*Solution*
We define and plot the Weibull log-likelihood and we get the MLE for the two parameters using `optim`.

```{r, echo=TRUE}

log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
       170.3, 148, 140, 118, 144, 97)
n <- length(y)


weib.y.mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)
parvalues <- expand.grid(gamma,beta)
llikvalues <- apply(parvalues, 1, log_lik_weibull, data=y)
llikvalues <- matrix(-llikvalues, nrow=length(gamma), ncol=length(beta),
                     byrow=F)
conf.levels <- c(0,0.5,0.75,0.9,0.95,0.99)

#contour plot
contour(gamma, beta, llikvalues-max(llikvalues),
        levels=-qchisq(conf.levels, 2)/2,
        xlab=expression(gamma),
        labels=as.character(conf.levels),
        ylab=expression(beta))
title('Weibull profile log-likelihood')

beta.gamma<- sapply(gamma,function(x) mean(y^x)^(1/x))
lines(gamma, beta.gamma, lty='dashed',col=2)
gammahat<-weib.y.mle$par[1]
betahat<-weib.y.mle$par[2]
points(weib.y.mle$par[1],weib.y.mle$par[2])
```

 We then compute the information matrix, which gives us the standard errors for the two parameters.
 
 
```{r, echo=TRUE}
#observed information matrix
jhat<-matrix(NA,nrow=2,ncol=2)
jhat[1,1]<-n/gammahat^2+sum((y/betahat)^gammahat*
                              (log(y/betahat))^2)
jhat[1,2]<-jhat[2,1]<- n/betahat-sum(y^gammahat/betahat^(gammahat+1)*
                                       (gammahat*log(y/betahat)+1))
jhat[2,2]<- -n*gammahat/betahat^2+gammahat*(gammahat+1)/
  betahat^(gammahat+2)*sum(y^gammahat)
solve(jhat)

mle.se<-sqrt(diag(solve(jhat)))
```
Now we plot the profile log-likelihood function with the Wald confidence interval of level 0.95.


```{r, echo=TRUE}
log_lik_weibull_profile  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
  log_lik_weibull( data, c(gamma, beta.gamma) )
}

log_lik_weibull_profile_v <-Vectorize(log_lik_weibull_profile, 'gamma'  )


plot(function(x) -log_lik_weibull_profile_v(data=y, x), 
from=0.1,to=15,xlab=expression(gamma),  ylab='profile relative log likelihood',ylim=c(-8,0))
w.ci1<-gammahat-qnorm(0.975)*mle.se[1]
w.ci2<-gammahat+qnorm(0.975)*mle.se[1]
#segments( w.ci1, , w.ci1, -log_lik_weibull_profile_v(y, w.ci1), col="red", lty=2  )
#segments( w.ci2,-qchisq(conf.level,1)/2, w.ci2, -log_lik_weibull_profile_v(y, w.ci2), col="red", lty=2  )
#points(w.ci1, , pch=16, col=2, cex=1.5)
#points(w.ci2, , pch=16, col=2, cex=1.5)
#segments( w.ci1, , w.ci2, , col="red", lty =1, lwd=2  )
#text(7,-7.5,"95% Wald-type CI",col=2)

```
 
**Exercise 3**

Repeat the steps above — write the profile log-likelihood, plot it and find the deviance confidence intervals — considering this time $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest.
 
**Exercise 5**

In `sim` in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of $\theta$. Please, produce an histogram for these random draws $\theta^{(1)},...,\theta^{(S)}$, compute the empirical quantiles, and overlap the true posterior distribution.
 
**Exercise 6**
 
Launch the following line of $R$ code:
```{}
posterior <- as.array(fit)
```
Use now the `bayesplot` package. Read the help and produce for this example, using the object posterior, the following plots:

* posterior intervals.
* posterior areas.
* marginal posterior distributions for the parameters.

**Quickly comment.**

**Exercise 7**

Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i \sim Poisson(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

1) $\pi(\lambda) = Beta(4,2)$;
2) $\pi(\lambda) = Normal(1,2)$;
3) $\pi(\lambda) = Gamma(4,2)$;

Now, compute your posterior as $\pi(\lambda | y) \propto L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analitically.
 
**Exercise 8**

Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the `rstan` library. Once you did it succesfully, open the file model called `biparametric.stan, and replace the line:

$$target+ = cauchy\_lpdf(sigma | 0, 2.5);$$

with the following one:

$$target+ = uniform\_lpdf(sigma | 0.1, 10);$$
Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.

**Exercise 9**

Reproduce the first plot above for the soccer goals, but this time by replacing Prior 1 with a $Gamma(2,4)$. Then, compute the final Bayes factor matrix (BR_matrix) with this new prior and the other ones unchanged, and comment. Is still Prior 2 favorable over all the others?
  

*Solution*
```{r, echo=TRUE}
library(LearnBayes)
data(soccergoals)

y <- soccergoals$goals

#write the likelihood function via the gamma distribution


lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

prior_norm <- function(npar, theta){
  lambda=exp(theta)  
  (dnorm(theta, npar[1], npar[2]))
  
}

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")


#likelihood
curve(lik_pois_v(theta=x, data=y), xlim=c(-1,4), xlab=expression(theta), ylab = "density", lwd =2 )
#prior 1 with parameters 2 and 4
curve(prior_gamma_v(theta=x, par=c(2, 4)), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
curve(prior_norm_v(theta=x, npar=c(1, .5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
curve(prior_norm_v(theta=x, npar=c(2, .5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
legend(2.6, 1.8, c("Lik.", "Ga(2,4)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
       lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```
This first prior is no more that similar to any of the other ones.

Let's compute the log-posteriors.

```{r, echo=TRUE}
 
logpoissongamma <- function(theta, datapar){
  data <- datapar$data
  par <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_gamma(par, theta))
  return(log_lik+log_prior)
}

logpoissongamma.v <- Vectorize( logpoissongamma, "theta")


logpoissonnormal <- function( theta, datapar){
  data <- datapar$data
  npar <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  
logpoissonnormal.v <- Vectorize( logpoissonnormal, "theta")

#log-likelihood
curve(log(lik_pois(y, theta=x)), xlim=c(-1,4),ylim=c(-20,2), lty =1,
      ylab="log-posteriors", xlab=expression(theta))
#log posterior 1
curve(logpoissongamma.v(theta=x, list(data=y, par=c(2, 4))), col="red", xlim=c(-1,4),ylim=c(-20,2), lty =1, add =TRUE)
#log posterior 2
curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(1, .5))), lty =1, col="blue",  add =TRUE)
#log posterior 3
curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(2, .5))), lty =1, col="green", add =TRUE, lwd =2)
#log posterior 4
curve(logpoissonnormal.v( theta=x, list(data=y, par=c(1, 2))), lty =1, col="violet", add =TRUE, lwd =2)
legend(2.6, 1.3, c( "loglik", "lpost 1", "lpost 2", "lpost 3", "lpost 4" ),
       lty=1, col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)

```

Now we want to compute the BF matrix in order to compare the prior distributions.

 ```{r, echo=TRUE}
datapar <- list(data=y, par=c(2, 4))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)
BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
    BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
    BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf
```

We still get that every prior is favored over Prior 3 and Prior 2 is still favorable over all the others. 
The only thing that changes is that now Prior 4 should be preferred to Prior 1.


**Exercise 10**

Let $y=(1,0,0,1,0,0,0,0,0,1,0,0,1,0)$ collect the results of tossing $n=14$ times an unfair coin, where 1 denotes heads and 0 tails, and $p=Prob(y_i=1)$

* Looking at the `Stan` code for the other models, write a short `Stan` Beta-Binomial model, where $p$ has a $Beta(a,b)$ prior with $a=3$, $b=3$;

* extract the posterior distribution with the function `extract()`;

* produce some plots with the `bayesplot` package and comment;

* compute analitically the posterior distribution and compare it with the `Stan` distribution.